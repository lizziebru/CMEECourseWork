# need a sequence of substrate concentrations rom 1-50 in jumps of 5
S_data <- seq(1,50,5)
S_data
# generate a Michaelis-Menten reaction velocity response with V_max = 12.5 and K_M = 7.1:
V_data <- ((12.5*S_data)/(7.1 + S_data))
plot(S_data, V_data)
plot(S_data, V_data)
# can now add some random (normally-distributed) fluctuations to the data to emulate experimental/measurement error:
set.seed(1456) # to get the same random fluctuations in the "data" every time
V_data <- V_data + rnorm(10, 0, 1) # add 10 random fluctuations with standard deviation of 0.5 to emulate error
plot(S_data, V_data)
plot(S_data, V_data)
MM_model <- nls(V_data ~ V_max * S_data / (K_M + S_data))
# for now: examine how good the fit we obtained looks:
plot(S_data,V_data, xlab = "Substrate Concentration", ylab = "Reaction Rate")  # first plot the data
# for now: examine how good the fit we obtained looks:
plot(S_data,V_data, xlab = "Substrate Concentration", ylab = "Reaction Rate")  # first plot the data
lines(S_data,predict(MM_model),lty=1,col="blue",lwd=2) # now overlay the fitted model
# to do this:
coef(MM_model) # check the coefficients
Substrate2Plot <- seq(min(S_data), max(S_data),len=200) # generate some new x-axis values just for plotting
Predict2Plot <- coef(MM_model)["V_max"] * Substrate2Plot / (coef(MM_model)["K_M"] + Substrate2Plot) # calculate the predicted values by plugging the fitted coefficients into the model equation
Substrate2Plot <- seq(min(S_data), max(S_data),len=200) # generate some new x-axis values just for plotting
Predict2Plot <- coef(MM_model)["V_max"] * Substrate2Plot / (coef(MM_model)["K_M"] + Substrate2Plot) # calculate the predicted values by plugging the fitted coefficients into the model equation
plot(S_data,V_data, xlab = "Substrate Concentration", ylab = "Reaction Rate")  # first plot the data
lines(Substrate2Plot, Predict2Plot, lty=1,col="blue",lwd=2) # now overlay the fitted model
summary(MM_model)
# confidence intervals
# --> useful to construct CIs around the estimated parameters in our fitted model
confint(MM_model)
# try the NLLS fitting again, this time with starting values:
MM_model2 <- nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 12, K_M = 7))
# compare the coefficient estimates from our two diff model fits to the same dataset:
coef(MM_model)
coef(MM_model2)
# trying even more different starting values:
MM_model3 <- nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = .01, K_M = 20))
# compare coefficients to the ones from the previous models
coef(MM_model)
coef(MM_model2)
coef(MM_model3)
# plotting model1 and model3 together to compare fit:
plot(S_data, V_data) # first plot the data
# plotting model1 and model3 together to compare fit:
plot(S_data, V_data) # first plot the data
lines(S_data, predict(MM_model), lty = 1, col = "blue", lwd = 2) # overlay the original model fit
lines(S_data, predict(MM_model3), lty = 1, col = "red", lwd = 2) # overlay the latest model fit
# trying again with even more different starting values:
nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0, K_M = 0.1))
# trying another pair of starting values:
nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = -0.1, K_M = 100))
# trying with values really close to the optimal values:
MM_model4 <_ nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 12.96, K_M = 10.61))
# trying with values really close to the optimal values:
MM_model4 <- nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 12.96, K_M = 10.61))
coeff(MM_model)
coeff(MM_model4)
coef(MM_model)
coef(MM_model4)
install.packages("minpack.lm")
require("minpack.lm")
MM_model5 <- nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 2, K_M = 2))
coef(MM_model2)
coef(MM_model5)
# now trying using all those starting parameter combinations that failed previously
MM_model6 <- nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = .01, K_M = 20))
MM_model7 <- nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0, K_M = 0.1))
MM_model8 <- nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = -0.1, K_M = 100))
coef(MM_model6)
coef(MM_model7)
coef(MM_model8)
# bu nlsLM also has limits:
# trying with more absurd starting values:
nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = -10, K_M = -100))
nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0.1, K_M = 0.1))
nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0.1, K_M = 0.1), lower=c(0.4,0.4), upper=c(100,100))
nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0.1, K_M = 0.1))
nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0.1, K_M = 0.1), lower=c(0.4,0.4), upper=c(100,100))
# if you bound the parameters too much, the algorithm can't search sufficient parameter space and will fail to converge on a good soln:
nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start =  list(V_max = 0.5, K_M = 0.5), lower=c(0.4,0.4), upper=c(20,20))
hist(residuals(MM_model6))
# further diagnostics: using the nlstools package
install.packages("nlstools")
require("nlstools")
confint2(MM_model6, level = 0.95)
# import data:
MyData <- read.csv("../data/GenomeSize.csv") # using relative path assuming that your working directory is "code"
# import data:
MyData <- read.csv("../data/GenomeSize.csv") # using relative path assuming that your working directory is "code"
setwd("~/Documents/CMEECourseWork/week4/code")
# import data:
MyData <- read.csv("../data/GenomeSize.csv")
head(MyData)
Data2Fit <- subset(MyData,Suborder == "Anisoptera")
# plot the data
plot(Data2Fit$TotalLength, Data2Fit$BodyWeight, xlab = "Body Length", ylab = "Body Weight")
# subset the data and remove NAs
Data2Fit <- subset(MyData,Suborder == "Anisoptera")
Data2Fit <- Data2Fit[!is.na(Data2Fit$TotalLength),] # remove NAs
# plot the data
plot(Data2Fit$TotalLength, Data2Fit$BodyWeight, xlab = "Body Length", ylab = "Body Weight")
# fit the model to the data using NLLS:
nrow(Data2Fit)
PowFit <- nlsLM(BodyWeight ~ a * TotalLength^b, data = Data2Fit, start = list(a = .1, b = .1))
# first create a function object for the power law model:
powMod <- function(x, a, b) {
return(a * x^b)
}
# now fit the model to the data using NLLS by calling the model:
PowFit <- nlsLM(BodyWeight ~ powMod(TotalLength,a,b), data = Data2Fit, start = list(a = .1, b = .1))
# now fit the model to the data using NLLS by calling the model:
PowFit <- nlsLM(BodyWeight ~ powMod(TotalLength,a,b), data = Data2Fit, start = list(a = .1, b = .1))
summary(PowFit)
Lengths <- seq(min(Data2Fit$TotalLength),max(Data2Fit$TotalLength),len=200)
coef(PowFit)["a"]
coef(PowFit)["b"]
Predic2PlotPow <- powMod(Lengths,coef(PowFit)["a"],coef(PowFit)["b"])
# plot the data and the fitted model line
plot(Data2Fit$TotalLength, Data2Fit$BodyWeight)
lines(Lengths, Predic2PlotPow, col = 'blue', lwd = 2.5)
summary(PowFit)
hist(residuals(PowFit))
allometric_ggplot <- ggplot()+
geom_point(Data2Fit, aes(x = TotalLength, y = BodyWeight))
require(ggplot2)
allometric_ggplot <- ggplot()+
geom_point(Data2Fit, aes(x = TotalLength, y = BodyWeight))
allometric_ggplot <- ggplot()+
geom_point(aes(x = Data2Fit$TotalLength, y = Data2Fit$BodyWeight))
allometric_ggplot
allometric_ggplot <- ggplot()+
geom_point(aes(x = Data2Fit$TotalLength, y = Data2Fit$BodyWeight))+
geom_smooth(Predic2PlotPow)
allometric_ggplot <- ggplot()+
geom_point(aes(x = Data2Fit$TotalLength, y = Data2Fit$BodyWeight))+
geom_smooth(aes(Predic2PlotPow))
allometric_ggplot
Predic2PlotPow <- powMod(Lengths,coef(PowFit)["a"],coef(PowFit)["b"])
Predic2PlotPow
allometric_ggplot <- ggplot()+
geom_point(aes(x = Data2Fit$TotalLength, y = Data2Fit$BodyWeight))+
geom_smooth(aes(x = Data2Fit$TotalLength, y = Predic2PlotPow))
allometric_ggplot
allometric_ggplot <- ggplot()+
geom_point(aes(x = Data2Fit$TotalLength, y = Data2Fit$BodyWeight))+
geom_smooth(aes(x = Lenths, y = Predic2PlotPow))
allometric_ggplot
allometric_ggplot <- ggplot()+
geom_point(aes(x = Data2Fit$TotalLength, y = Data2Fit$BodyWeight))+
geom_smooth(aes(x = Lengths, y = Predic2PlotPow))
allometric_ggplot
allometric_ggplot <- ggplot()+
geom_point(aes(x = Data2Fit$TotalLength, y = Data2Fit$BodyWeight))+
geom_smooth(aes(x = Lengths, y = Predic2PlotPow))+
xlab("Total Length")+
ylab("Body Weight")
allometric_ggplot
require(ggpmisc)
install.packages("ggpmisc")
install.packages("ggpmisc")
require(ggpmisc)
require(minpack.lm)
require(nlstools)
require(ggplot2)
rm(list = ls())
graphics.off()
# need a sequence of substrate concentrations rom 1-50 in jumps of 5
S_data <- seq(1,50,5)
rm(list = ls())
graphics.off()
# need a sequence of substrate concentrations rom 1-50 in jumps of 5
S_data <- seq(1,50,5)
S_data
# generate a Michaelis-Menten reaction velocity response with V_max = 12.5 and K_M = 7.1:
V_data <- ((12.5*S_data)/(7.1 + S_data))
plot(S_data, V_data)
# can now add some random (normally-distributed) fluctuations to the data to emulate experimental/measurement error:
set.seed(1456) # to get the same random fluctuations in the "data" every time
V_data <- V_data + rnorm(10, 0, 1) # add 10 random fluctuations with standard deviation of 0.5 to emulate error
plot(S_data, V_data)
MM_model <- nls(V_data ~ V_max * S_data / (K_M + S_data))
# for now: examine how good the fit we obtained looks:
plot(S_data,V_data, xlab = "Substrate Concentration", ylab = "Reaction Rate")  # first plot the data
lines(S_data,predict(MM_model),lty=1,col="blue",lwd=2) # now overlay the fitted model
coef(MM_model) # check the coefficients
Substrate2Plot <- seq(min(S_data), max(S_data),len=200) # generate some new x-axis values just for plotting
Predict2Plot <- coef(MM_model)["V_max"] * Substrate2Plot / (coef(MM_model)["K_M"] + Substrate2Plot) # calculate the predicted values by plugging the fitted coefficients into the model equation
plot(S_data,V_data, xlab = "Substrate Concentration", ylab = "Reaction Rate")  # first plot the data
lines(Substrate2Plot, Predict2Plot, lty=1,col="blue",lwd=2) # now overlay the fitted model
summary(MM_model)
# confidence intervals
# --> useful to construct CIs around the estimated parameters in our fitted model
confint(MM_model)
# try the NLLS fitting again, this time with starting values:
MM_model2 <- nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 12, K_M = 7))
# compare the coefficient estimates from our two diff model fits to the same dataset:
coef(MM_model)
coef(MM_model2)
# trying even more different starting values:
MM_model3 <- nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = .01, K_M = 20))
# compare coefficients to the ones from the previous models
coef(MM_model)
coef(MM_model2)
coef(MM_model3)
# plotting model1 and model3 together to compare fit:
plot(S_data, V_data) # first plot the data
lines(S_data, predict(MM_model), lty = 1, col = "blue", lwd = 2) # overlay the original model fit
lines(S_data, predict(MM_model3), lty = 1, col = "red", lwd = 2) # overlay the latest model fit
# trying again with even more different starting values:
nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0, K_M = 0.1))
# trying another pair of starting values:
nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = -0.1, K_M = 100))
# trying with values really close to the optimal values:
MM_model4 <- nls(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 12.96, K_M = 10.61))
coef(MM_model)
coef(MM_model4)
MM_model5 <- nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 2, K_M = 2))
coef(MM_model2)
coef(MM_model5)
# now trying using all those starting parameter combinations that failed previously
MM_model6 <- nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = .01, K_M = 20))
MM_model7 <- nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0, K_M = 0.1))
MM_model8 <- nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = -0.1, K_M = 100))
coef(MM_model6)
coef(MM_model7)
coef(MM_model8)
# bu nlsLM also has limits:
# trying with more absurd starting values:
nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = -10, K_M = -100))
nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0.1, K_M = 0.1))
# now the same with parameter bounds:
nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start = list(V_max = 0.1, K_M = 0.1), lower=c(0.4,0.4), upper=c(100,100))
# if you bound the parameters too much, the algorithm can't search sufficient parameter space and will fail to converge on a good soln:
nlsLM(V_data ~ V_max * S_data / (K_M + S_data), start =  list(V_max = 0.5, K_M = 0.5), lower=c(0.4,0.4), upper=c(20,20))
hist(residuals(MM_model6))
# e.g. can get confidence intervals:
confint2(MM_model6, level = 0.95)
# import data:
MyData <- read.csv("../data/GenomeSize.csv")
head(MyData)
# subset the data and remove NAs
Data2Fit <- subset(MyData,Suborder == "Anisoptera")
Data2Fit <- Data2Fit[!is.na(Data2Fit$TotalLength),] # remove NAs
# plot the data
plot(Data2Fit$TotalLength, Data2Fit$BodyWeight, xlab = "Body Length", ylab = "Body Weight")
# fit the model to the data using NLLS:
nrow(Data2Fit)
PowFit <- nlsLM(BodyWeight ~ a * TotalLength^b, data = Data2Fit, start = list(a = .1, b = .1))
# first create a function object for the power law model:
powMod <- function(x, a, b) {
return(a * x^b)
}
# now fit the model to the data using NLLS by calling the model:
PowFit <- nlsLM(BodyWeight ~ powMod(TotalLength,a,b), data = Data2Fit, start = list(a = .1, b = .1))
summary(PowFit)
Lengths <- seq(min(Data2Fit$TotalLength),max(Data2Fit$TotalLength),len=200)
coef(PowFit)["a"] # need to extract the coefficient from the model fit object using the coef() command
coef(PowFit)["b"]
Predic2PlotPow <- powMod(Lengths,coef(PowFit)["a"],coef(PowFit)["b"])
# plot the data and the fitted model line
plot(Data2Fit$TotalLength, Data2Fit$BodyWeight)
lines(Lengths, Predic2PlotPow, col = 'blue', lwd = 2.5)
summary(PowFit)
print(confint(PowFit))
hist(residuals(PowFit))
eqn <-
allometric_ggplot <- ggplot()+
geom_point(aes(x = Data2Fit$TotalLength, y = Data2Fit$BodyWeight))+
geom_smooth(aes(x = Lengths, y = Predic2PlotPow))+
xlab("Total Length")+
ylab("Body Weight")+
s
allometric_ggplot
PowFit2 <- nlsLM(BodyWeight ~ powMod(TotalLength,a,b), data = Data2Fit, start = list(a = .001, b = .001))
nlsLM(BodyWeight ~ powMod(TotalLength,a,b), data = Data2Fit, start = list(a = .001, b = .001))
nlsLM(BodyWeight ~ powMod(TotalLength,a,b), data = Data2Fit, start = list(a = 0, b = .001))
nlsLM(BodyWeight ~ powMod(TotalLength,a,b), data = Data2Fit, start = list(a = 0, b = 0))
nlsLM(BodyWeight ~ powMod(TotalLength,a,b), data = Data2Fit, start = list(a = .1, b = -0.1))
install.packages('IRkernel')
IRkernel::installspec()
jupyter labextension install @techrah/text-shortcuts
